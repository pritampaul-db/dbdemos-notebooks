{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df4f4f6b-63c0-4b7b-a66c-a0fff1bb7eef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Agent notebook\n",
    "\n",
    "This is an auto-generated notebook created by an AI Playground export. We generated three notebooks in the same folder:\n",
    "- [**agent**]($./agent): contains the code to build the agent.\n",
    "- [config.yml]($./config.yml): contains the configurations.\n",
    "- [driver]($./driver): logs, evaluate, registers, and deploys the agent.\n",
    "\n",
    "This notebook uses Mosaic AI Agent Framework ([AWS](https://docs.databricks.com/en/generative-ai/retrieval-augmented-generation.html) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/retrieval-augmented-generation)) to recreate your agent from the AI Playground. It defines a LangChain agent that has access to the tools from the source Playground session.\n",
    "\n",
    "Use this notebook to iterate on and modify the agent. For example, you could add more tools or change the system prompt.\n",
    "\n",
    " **_NOTE:_**  This notebook uses LangChain, however AI Agent Framework is compatible with other agent frameworks like Pyfunc and LlamaIndex.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Review the contents of [config.yml]($./config.yml) as it defines the tools available to your agent, the LLM endpoint, and the agent prompt.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "After testing and iterating on your agent in this notebook, go to the auto-generated [driver]($./driver) notebook in this folder to log, register, and deploy the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9572094b-30e5-4960-8e3d-a0052938a2c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow-skinny langchain==0.2.16 langchain_core langchain-community==0.2.16 langgraph==0.2.16 pydantic\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08c71597-b9e5-4754-ab5a-9ccbf9fe3765",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import and setup\n",
    "\n",
    "Use `mlflow.langchain.autolog()` to set up [MLflow traces](https://docs.databricks.com/en/mlflow/mlflow-tracing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c427186-f32d-43e6-87ca-43502313eafd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import ModelConfig\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "config = ModelConfig(development_config=\"config.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cbbf340-a8bd-4859-8d68-b698cdf62cbd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define the chat model and tools\n",
    "Create a LangChain chat model that supports [LangGraph tool](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/) calling.\n",
    "\n",
    "Modify the tools your agent has access to by modifying the `uc_functions` list in [config.yml]($./config.yml). Any non-UC function spec tools can be defined in this notebook. See [LangChain - How to create tools](https://python.langchain.com/v0.2/docs/how_to/custom_tools/) and [LangChain - Using built-in tools](https://python.langchain.com/v0.2/docs/how_to/tools_builtin/).\n",
    "\n",
    " **_NOTE:_**  This notebook uses LangChain, however AI Agent Framework is compatible with other agent frameworks like Pyfunc and LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9ee8192-c563-43d2-afa8-fb8f25278c87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain_community.tools.databricks import UCFunctionToolkit\n",
    "\n",
    "# Create the llm\n",
    "llm = ChatDatabricks(endpoint=config.get(\"llm_endpoint\"))\n",
    "\n",
    "uc_functions = config.get(\"uc_functions\")\n",
    "\n",
    "uc_function_tools = (\n",
    "    UCFunctionToolkit(warehouse_id=config.get(\"warehouse_id\"))\n",
    "    .include(*uc_functions)\n",
    "    .get_tools()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a135a94d-ade9-4a62-8a03-8a813d66f3c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Output parsers\n",
    "Databricks interfaces, such as the AI Playground, can optionally display pretty-printed tool calls.\n",
    "\n",
    "Use the following helper functions to parse the LLM's output into the expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b4500a3-205c-45fe-8614-79a03321404c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterator, Dict, Any\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    ToolMessage,\n",
    "    MessageLikeRepresentation,\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "uc_functions_set = {x.replace(\".\", \"__\") for x in uc_functions}\n",
    "\n",
    "\n",
    "def is_uc_function(tool_name: str) -> bool:\n",
    "    \"\"\"Check if `tool_name` is in `uc_functions` or belongs to a schema from config.yml.\"\"\"\n",
    "    if tool_name in uc_functions_set:\n",
    "        return True\n",
    "    for pattern in uc_functions_set:\n",
    "        if \"*\" in pattern and tool_name.startswith(pattern[:-1]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def stringify_tool_call(tool_call: Dict[str, Any]) -> str:\n",
    "    \"\"\"Convert a raw tool call into a formatted string that the playground UI expects\"\"\"\n",
    "    if is_uc_function(tool_call.get(\"name\")):\n",
    "        request = json.dumps(\n",
    "            {\n",
    "                \"id\": tool_call.get(\"id\"),\n",
    "                \"name\": tool_call.get(\"name\"),\n",
    "                \"arguments\": json.dumps(tool_call.get(\"args\", {})),\n",
    "            },\n",
    "            indent=2,\n",
    "        )\n",
    "        return f\"<uc_function_call>{request}</uc_function_call>\"\n",
    "    else:\n",
    "        # for non UC functions, return the string representation of tool calls\n",
    "        # you can modify this to return a different format\n",
    "        return str(tool_call)\n",
    "\n",
    "\n",
    "def stringify_tool_result(tool_msg: ToolMessage) -> str:\n",
    "    \"\"\"Convert a ToolMessage into a formatted string that the playground UI expects\"\"\"\n",
    "    if is_uc_function(tool_msg.name):\n",
    "        result = json.dumps(\n",
    "            {\"id\": tool_msg.tool_call_id, \"content\": tool_msg.content}, indent=2\n",
    "        )\n",
    "        return f\"<uc_function_result>{result}</uc_function_result>\"\n",
    "    else:\n",
    "        # for non UC functions, return the string representation of tool message\n",
    "        # you can modify this to return a different format\n",
    "        return str(tool_msg)\n",
    "\n",
    "\n",
    "def parse_message(msg) -> str:\n",
    "    \"\"\"Parse different message types into their string representations\"\"\"\n",
    "    # tool call result\n",
    "    if isinstance(msg, ToolMessage):\n",
    "        return stringify_tool_result(msg)\n",
    "    # tool call\n",
    "    elif isinstance(msg, AIMessage) and msg.tool_calls:\n",
    "        tool_call_results = [stringify_tool_call(call) for call in msg.tool_calls]\n",
    "        return \"\".join(tool_call_results)\n",
    "    # normal HumanMessage or AIMessage (reasoning or final answer)\n",
    "    elif isinstance(msg, (AIMessage, HumanMessage)):\n",
    "        return msg.content\n",
    "    else:\n",
    "        print(f\"Unexpected message type: {type(msg)}\")\n",
    "        return str(msg)\n",
    "\n",
    "\n",
    "def wrap_output(stream: Iterator[MessageLikeRepresentation]) -> Iterator[str]:\n",
    "    \"\"\"\n",
    "    Process and yield formatted outputs from the message stream.\n",
    "    The invoke and stream langchain functions produce different output formats.\n",
    "    This function handles both cases.\n",
    "    \"\"\"\n",
    "    for event in stream:\n",
    "        # the agent was called with invoke()\n",
    "        if \"messages\" in event:\n",
    "            for msg in event[\"messages\"]:\n",
    "                yield parse_message(msg) + \"\\n\\n\"\n",
    "        # the agent was called with stream()\n",
    "        else:\n",
    "            for node in event:\n",
    "                for key, messages in event[node].items():\n",
    "                    if isinstance(messages, list):\n",
    "                        for msg in messages:\n",
    "                            yield parse_message(msg) + \"\\n\\n\"\n",
    "                    else:\n",
    "                        print(\"Unexpected value {messages} for key {key}. Expected a list of `MessageLikeRepresentation`'s\")\n",
    "                        yield str(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aac2c2f0-59ef-4548-9651-348f8172a069",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create the agent\n",
    "Use the LangGraph [`create_react_agent` function](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/#usage) to build a simple graph with the model and tools defined by [config.yml]($./config.yml). For more customization, you can create your own LangGraph agent by following [LangGraph - Quick Start](https://langchain-ai.github.io/langgraph/tutorials/introduction/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4e3e72-a58f-4d8d-a9ba-4f87919dd824",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableGenerator\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Create the agent\n",
    "system_message = config.get(\"agent_prompt\")\n",
    "agent_with_raw_output = create_react_agent(llm, uc_function_tools, state_modifier=system_message)\n",
    "agent = agent_with_raw_output | RunnableGenerator(wrap_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff26937c-a08c-4aa0-a655-da770aa412a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Test the agent\n",
    "\n",
    "Interact with the agent to test its output. Since this notebook called `mlflow.langchain.autolog()` you can view the trace for each step the agent takes.\n",
    "\n",
    "Replace this placeholder input with an appropriate domain-specific example for your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5408b98f-6ecd-4d0b-9a1e-d4acdfd849d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Input example with an appropriate domain-specific example for your agent\n",
    "for event in agent.stream({\"messages\": [{\"role\": \"user\", \"content\": \"what is the total number of reviews for the product  Stone Grenade?\"}]}):\n",
    "    print(event, \"---\" * 20 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad777788-44d5-4bde-848f-0d601d443b2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.models.set_model(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3522a1b6-0a8f-4536-abea-fe62b6b18303",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "You can rerun the cells above to iterate and test the agent.\n",
    "\n",
    "Go to the auto-generated [driver]($./driver) notebook in this folder to log, register, and deploy the agent."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "boat_agent",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
